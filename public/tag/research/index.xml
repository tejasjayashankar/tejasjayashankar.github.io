<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Research | Tejas Jayashankar</title>
    <link>https://tejasjayashankar.github.io/tag/research.html</link>
      <atom:link href="https://tejasjayashankar.github.io/tag/research/index.xml" rel="self" type="application/rss+xml" />
    <description>Research</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 01 May 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://tejasjayashankar.github.io/media/logo_hud022c539867be043536a32a33d2fd77b_252126_300x300_fit_lanczos_3.png</url>
      <title>Research</title>
      <link>https://tejasjayashankar.github.io/tag/research.html</link>
    </image>
    
    <item>
      <title>Defended PhD thesis at MIT</title>
      <link>https://tejasjayashankar.github.io/news/2025-defended-phd.html</link>
      <pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate>
      <guid>https://tejasjayashankar.github.io/news/2025-defended-phd.html</guid>
      <description>&lt;p&gt;I successfully defended my PhD thesis titled &amp;ldquo;Score Estimation for Generative Modeling&amp;rdquo; in the Department of Electrical Engineering and Computer Science at MIT, advised by Professor Gregory W. Wornell. The thesis addresses three primary areas within score-based and diffusion generative models: improved score estimation techniques, Bayesian-inspired frameworks for solving inverse problems such as signal separation, and efficient one-step generation via mixture distribution score estimation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Score-of-Mixture Training accepted as Spotlight Poster (top 2%) at ICML 2025</title>
      <link>https://tejasjayashankar.github.io/news/2025-smt.html</link>
      <pubDate>Tue, 01 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://tejasjayashankar.github.io/news/2025-smt.html</guid>
      <description>&lt;p&gt;Our new paper &amp;ldquo;Score-of-Mixture Training: One-Step Generative Model Training Made Simple via Score Estimation of Mixture Distributions&amp;rdquo; has been accepted as a &lt;strong&gt;Spotlight Poster (top 2%)&lt;/strong&gt; at the 42nd International Conference on Machine Learning (ICML 2025). This is joint work with Jongha (Jon) Ryu and Gregory W. Wornell. Please refer to the links above for more information.  In this work we propose a new
distribution matching framework grounded in minimizing the skewed Jensen-Shannon divergence between interpolated distributions that we show allows for a simple framework for training generative models from scratch (without expensive pre-training and distillation), with minimal architectural design changes and outperforms existing methods for image generative modeling from scratch.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We propose Score-of-Mixture Training (SMT), a novel framework for training one-step generative models by minimizing a class of divergences called the $\alpha$-skew Jensen&amp;ndash;Shannon divergence. At its core, SMT estimates the score of mixture distributions between real and fake samples across multiple noise levels. Similar to consistency models, our approach supports both training from scratch (SMT) and distillation using a pretrained diffusion model, which we call Score-of-Mixture Distillation (SMD). It is simple to implement, requires minimal hyperparameter tuning, and ensures stable training. Experiments on CIFAR-10 and ImageNet 64x64 show that SMT/SMD are competitive with and can even outperform existing methods.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recent work on improved score-estimation for diffusion models and implicit models accepted at ICML SPIGM Workshop 2024!</title>
      <link>https://tejasjayashankar.github.io/news/2024-lifted-sm.html</link>
      <pubDate>Mon, 01 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://tejasjayashankar.github.io/news/2024-lifted-sm.html</guid>
      <description>&lt;p&gt;Score estimators lie at the heart of modern generative models, enabling the design and
training of the latest generation of diffusion models based on Gaussian noise corruption
processes.  In this work we propose a new technique for learning the score of an
underlying probability measure by defining a new objective in the lifted space of matrix
valued objects.  We show that this new objective can be optimized without any additional
computational or memory overhead than existing score matching objectives.  Moreover, we
empirically demonstrate that the resulting method can lead to improvements in FID for
generative modeling using diffusion models on the CIFAR-10 dataset and for implicit
models trained on the CelebA dataset.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This paper proposes two new techniques to improve the accuracy of score estimation. The first
proposal is a new objective function called the lifted score estimation objective, which serves as
a replacement for the score matching (SM) objective. Instead of minimizing the expected l2-distance
between the learned and true score models, the proposed objective operates in the lifted
space of the outer-product of a vector-valued function with itself. The distance is defined as the expected squared Frobenius norm of the difference between such matrix-valued objects induced by the learned and true score functions. The second
idea is to model and learn the residual approximation error of the learned score estimator, given
a base score model architecture. We empirically demonstrate that the combination of the two ideas
called lifted residual score estimation outperforms sliced SM in training VAE and WAE with implicit
encoders, and denoising SM in training diffusion models, as evaluated by downstream metrics of
sample quality such as the FID score.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Latest paper on score-based methods for single-channel source separation accepted at NeurIPS 2023</title>
      <link>https://tejasjayashankar.github.io/news/2023-score-based-scss.html</link>
      <pubDate>Wed, 01 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://tejasjayashankar.github.io/news/2023-score-based-scss.html</guid>
      <description>&lt;p&gt;Our latest paper on score-based methods for source separation with applications to
digital communication signals with underlying discrete structures was accepted for a
poster presentation at NeurIPS 2023. Please refer the abstract pasted below and to the
links above for more information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We propose a new method for separating superimposed sources using diffusion-based generative models. Our method relies only on separately trained statistical priors of independent sources to establish a new objective function guided by maximum a posteriori estimation with an $\alpha$-posterior, across multiple levels of Gaussian smoothing. Motivated by applications in radio-frequency (RF) systems, we are interested in sources with underlying discrete nature and the recovery of encoded bits from a signal of interest, as measured by the bit error rate (BER). Experimental results with RF mixtures demonstrate that our method results in a BER reduction of 95% over classical and existing learning-based methods. Our analysis demonstrates that our proposed method yields solutions that asymptotically approach the modes of an underlying discrete distribution. Furthermore, our method can be viewed as a multi-source extension to the recently proposed score distillation sampling scheme, shedding additional light on its use beyond conditional sampling&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Wrapped up my PhD Student Researcher Position at Google Research</title>
      <link>https://tejasjayashankar.github.io/news/2023-google-research.html</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      <guid>https://tejasjayashankar.github.io/news/2023-google-research.html</guid>
      <description>&lt;p&gt;Wrapped up my internship at Google Research where I worked with Dr. Fabian Mentzer on
the Neural Compression team on transformer-based video compression architectures. Please
refer to slides for a summary of our results.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
