---


# Display name
title: Tejas Jayashankar

# Full name (for SEO)
first_name: Tejas
last_name: Jayashankar

# Status emoji
status:
  icon:

# Is this the primary user of the site?
superuser: true

# Role/position/tagline
role: Research Scientist

# Organizations/Affiliations to show in About widget
organizations:
  - name: Meta Superintelligence Labs (MSL)

# Short bio (displayed in user profile at end of posts)
bio: I am a research scientist at Meta Superintelligence Labs working on diffusion models,
  few-step generative modeling for speech synthesis and post-training of multimodal LLMs

# Interests to show in About widget
interests:
  - Few-step Generative modeling
  - Score Estimation and Diffusion Models
  - (Neural) Data Compression
  - Representation Learning
  - Information Theory

# Education to show in About widget
education:
  courses:
    - course: PhD in Electrical Engineering & Computer Science
      institution: Massachusetts Institute of Technology (MIT)
      year: 2022 - 2025
    - course: Masters of Science in Electrical Engineering & Computer Science
      institution: Massachusetts Institute of Technology (MIT)
      year: 2019 - 2022
    - course: Bachelors of Science in Electrical Engineering
      institution: University of Illinois at Urbana-Champaign (UIUC)
      year: 2015 - 2019

# Social/Academic Networking
# For available icons, see: https://wowchemy.com/docs/getting-started/page-builder/#icons
#   For an email link, use "fas" icon pack, "envelope" icon, and a link in the
#   form "mailto:your-email@example.com" or "/#contact" for contact widget.
# Alternatively, use `google-scholar` icon from `ai` icon pack
social:
  - icon: envelope
    icon_pack: fas
    link: mailto:tejasj@mit.edu
    # link: '/#contact'
  - icon: linkedin
    icon_pack: fab
    link: https://www.linkedin.com/in/tkj97/
    label: Follow me on LinkedIn
  # - icon: twitter
  #   icon_pack: fab
  #   link: https://twitter.com/TejasJayashank2
  #   label: Follow me on X
  - icon: google-scholar
    icon_pack: ai
    link: https://scholar.google.com/citations?user=gjN_lUoAAAAJ&hl=en
  - icon: github
    icon_pack: fab
    link: https://github.com/tkj516
    display:
      header: true
  - icon: cv
    icon_pack: ai
    link: https://drive.google.com/file/d/1KJgmH4n_WFRHt30PSur7rr1O_Wdn5x0s/view?usp=sharing

# Enter email to display Gravatar (if Gravatar enabled in Config)
email: ''

# Highlight the author in author lists? (true/false)
highlight_name: true
---

**About Me**

I am a Research Scientist at Meta Superintelligence Labs (MSL) where I currently focus on 
speech synthesis from Meta's multimodal models, particularly focusing on diffusion modeling,
few-step generative modeling for speech synthensis, neural codec training and multimodal 
LLM post-training with a focus on reward modeling for RLHF.   

I was previously a PhD student in the [EECS department at MIT](https://www.eecs.mit.edu/) 
advised by [Professor Gregory Wornell](http://allegro.mit.edu/~gww/). I defended my Ph.D.
thesis titled 
["Score Estimation for Generative Modeling"](https://dspace.mit.edu/handle/1721.1/164063) in May 2025.  Before that I completed my
 M.S. degree in EECS from MIT in January 2022 and my B.S. degree in Electrical Engineering from 
the [University of Illinois at Urbana-Champaign](https://ece.illinois.edu/) in 2019. 

**Current Research Interests**

My current research interests include:

- **Efficient Generative Modeling and Score Estimation:** I conduct foundational 
  research on score estimation, flow matching and distribution matching to develop 
  new algorithms to improve training and sampling efficiency in generative models (e.g., 
  [ICML '25 Spotlight](https://openreview.net/forum?id=zk5k2NQcEA)) and solving 
  inverse problems (e.g., [NeurIPS '23](https://alpha-rgs.github.io/)).

- **Reward Modeling and RLHF:** At MSL, I focus on training reward models—both 
  preference-based and generative—to enhance the aesthetic quality of multimodal 
  LLM outputs.

- **Neural Data Compression:** I explore how generative models can serve as density 
  estimators and powerful priors to advance low-bitrate compression for source coding. 
  This work has applications in variable-rate, successively refinable data tokenization. 
  My previous contributions include research at 
  [Google Research](https://docs.google.com/presentation/d/12mXjO50qHUXiuk062CGNG6gNwtfHcO86uYLYbCLQmms/edit?usp=sharing&resourcekey=0-onMp2_GuNQTqEIEbvpB1AA), 
  [ICASSP '22](https://ieeexplore.ieee.org/abstract/document/9747419), and a 
  [patented model-code separation architecture](https://dspace.mit.edu/handle/1721.1/148612) 
  developed at MIT.

